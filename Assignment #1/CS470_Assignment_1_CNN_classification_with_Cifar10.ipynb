{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS470 Assignment #1: CNN classification with Cifar10",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRqPMAzzNipD",
        "colab_type": "text"
      },
      "source": [
        "CS470 Assignment #1: CNN classification with Cifar10\n",
        "====\n",
        "\n",
        "Primary TA : Myeongjae Jang\n",
        "\n",
        "TA's E-mail : myeongjae0409@kaist.ac.kr\n",
        "\n",
        "## Instruction\n",
        "\n",
        "- Modify the baseline CNN model to improve the classification performance on Cifar10 dataset. In addition to the model definition, you can modify any parts of this colab example to improve the test accuracy (e.g., learning rate, batch size, etc.)\n",
        "- Train your CNN model and compare it to the baseline (in terms of training loss and the test accuracy).\n",
        "- Explain your modifications and discuss how you improved the test accuracy.\n",
        "\n",
        "## Submission guidelines\n",
        "\n",
        "- Your code and report will be all in Colab. Copy this example to your google drive and edit it to complete your assignment. Add sections at the bottom of this example to discuss the results. For discussion and analysis, we highly encourage you to use graphics if possible (e.g., plots, images, etc.). \n",
        "- To make grading efficient, please highlight all contributions & modifications you made clearly. We highly encourage you to add code blocks in the discussion section to discuss your modifications (e.g., you can describe the model definition in the discussion section using the code blocks).\n",
        "- We should be able to reproduce your results using your code and pre-trained model. Please double-check if your code runs without error and loads your pre-trained model properly. Submissions failed to run or reproduce the results will get a substantial penalty. \n",
        "- In this assignment, **we are not allowing fine-tuning from the pre-trained model** (e.g. ImageNet pre-trained models). You should train your  model on Cifar10 dataset from scratch. \n",
        "\n",
        "## Deliverables\n",
        "- Download your Colab notebook and the pre-trained model, and submit a zip file in a format: [StudentID].zip. Please double-check that you locate and load your pre-trained model properly.\n",
        "- Your assignment should be submitted through KLMS. All other submissions (e.g., via email) will not be considered as valid submissions. \n",
        "\n",
        "## Grading policy\n",
        "\n",
        "- **Code** (50%): Your code should work and outperform the baseline model in terms of the test accuracy. \n",
        "- **Report** (50%): Explain your modification and justify how it improved the perofrmance. It would be great if you have some supporting results for your justification (e.g., justifying that you resolved the overfitting by comparing two training/testing loss curves). \n",
        "- **Extra points** will be given if your submission satisfies the following:\n",
        " - **High test accuracy**: we will rank the submissions based on the test accuracy, and assign extra points according to the rank (e.g. 3 points for top 10%, 2 points for top 30%, 1 points for top 50%.)\n",
        " - **Comprehensive discussion**: we will assign extra points if your report contains comprehensive discussion/analysis of the results. Examples include justification of your choice of model (or hyper-parameters), comparisons to the baseline model (analysis on the source of improvement), insightful visualizations (loss curves, misclassification results), etc.\n",
        "\n",
        "## Due date\n",
        "- **23:59:59 September 25th.**\n",
        "- Late submission is allowed until 23:59:59 September 27th.\n",
        "- Late submission will be applied 20% penalty.\n",
        "\n",
        "## Questions\n",
        "- Please use QnA board in KLMS as a main communication channel. When you post questions, please make it public so that all students can share the information. Please use the prefix \"[Assignment 1]\" in the subject for all questions regarding this assignment (e.g., [Assignment 1] Regarding the grading policy).\n",
        "\n",
        "## PyTorch Documentation\n",
        "- You can refer PyTorch documentation for your assignment.\n",
        "- https://pytorch.org/docs/stable/index.html\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RO1mgGV_uOIK",
        "colab_type": "text"
      },
      "source": [
        "## Step 1: Connect to your Google Drive\n",
        "\n",
        "It is required if you want to save checkpoints and load them later on.\n",
        "\n",
        "### (You have to submit your trained results as the checkpoint. So, please check your Google Drive connection again.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cLth6ZfXuSGT",
        "colab_type": "code",
        "outputId": "6d89f435-521e-4df4-9bbb-3a841d221e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/gdrive')\n",
        "gdrive_root = '/gdrive/My Drive'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /gdrive; to attempt to forcibly remount, call drive.mount(\"/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RYwUwGf8qW1U",
        "colab_type": "text"
      },
      "source": [
        "## Step 2: Import modules"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UtshANjqpy4",
        "colab_type": "code",
        "outputId": "0ce40d7f-3241-4040-ce6b-a6b80cf2a3e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import os\n",
        "\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "!pip install -U tensorboardcolab\n",
        "from tensorboardcolab import TensorBoardColab\n",
        "\n",
        "torch.manual_seed(470)\n",
        "torch.cuda.manual_seed(470)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: tensorboardcolab in /usr/local/lib/python3.6/dist-packages (0.0.22)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0iJ-Q6sbq8c3",
        "colab_type": "text"
      },
      "source": [
        "## Step 3: Configure the experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA5jAy7Wq-E2",
        "colab_type": "code",
        "outputId": "1ff8c609-ca2c-44e4-dae4-6668717f8b86",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# My modification\n",
        "# batch_size = 20000 <--- Original code\n",
        "batch_size = 18\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Wait for 8 seconds...\n",
            "TensorBoard link:\n",
            "https://24e99c67.ngrok.io\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tZt60aMrQ1g",
        "colab_type": "text"
      },
      "source": [
        "## Step 4: Construct data pipeline\n",
        "\n",
        "**`torchvision.datasets.CIFAR10`** will automatically construct **`Cifar10`** dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHbtV46LrXOF",
        "colab_type": "code",
        "outputId": "8ace8499-8c23-4f29-ff7e-fe664f6c2347",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "data_dir = os.path.join(gdrive_root, 'my_data')\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "train_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=True, download=True, transform=transform)\n",
        "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "\n",
        "test_dataset = torchvision.datasets.CIFAR10(root=data_dir, train=False, download=True, transform=transform)\n",
        "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6G_dWd-6rwWb",
        "colab_type": "text"
      },
      "source": [
        "## Step 5: Construct a neural network builder\n",
        "\n",
        "We serve the baseline CNN model which is supported on Pytorch tutorial: https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/cifar10_tutorial.ipynb#scrollTo=c1E1b7-igUcR\n",
        "\n",
        "### (You have to compare your own CNN model's test accuracy with the baseline CNN model and explain why your own model's test accuracy is higher than the basline.)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FX_wne0Vr1E5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MyClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features=6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 1 * 1, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(self.relu(self.batchnorm1(self.conv1(x))))\n",
        "      x = self.pool(self.relu(self.conv2(x)))\n",
        "      x = self.pool(self.relu(self.batchnorm3(self.conv3(x))))\n",
        "      x = self.pool(self.relu(self.conv4(x)))\n",
        "      x = x.view(-1, 64 * 1 * 1)\n",
        "      x = self.relu(self.fc1(x))\n",
        "      outputs = self.fc2(x)\n",
        "      return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z5M7IhbmeguX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpA3xhjMspvA",
        "colab_type": "text"
      },
      "source": [
        "## Step 6: Initialize the network and optimizer\n",
        "\n",
        "If you want to train modularized neural network in Step 5B, please use 'MyClassifier2' as 'my_classifier'. It is written as a comment now."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XP111gW0s8aH",
        "colab_type": "code",
        "outputId": "57bed942-71fd-49df-83b6-43275851cf69",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "my_classifier = MyClassifier()\n",
        "my_classifier = my_classifier.to(device)\n",
        "\n",
        "# Print your neural network structure\n",
        "print(my_classifier)\n",
        "\n",
        "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MyClassifier(\n",
            "  (conv1): Conv2d(3, 6, kernel_size=(5, 5), stride=(1, 1))\n",
            "  (batchnorm1): BatchNorm2d(6, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (relu): ReLU()\n",
            "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (batchnorm3): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "  (conv4): Conv2d(32, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=64, out_features=64, bias=True)\n",
            "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lAQeXmjsILS",
        "colab_type": "text"
      },
      "source": [
        "## Step 7: Load pre-trained weights if exist\n",
        "\n",
        "- **For your submission you have to store the trained model as a checkpoint.**\n",
        "- Please do not erase this step.\n",
        "- If you want to modify this step, please be careful.\n",
        "- After training please confirm that your checkpoint is correctly stored and re-loaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFLNZxaBsHUl",
        "colab_type": "code",
        "outputId": "386adfbf-97aa-44b4-aff5-68b6b3fba189",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "ckpt_dir = os.path.join(gdrive_root, 'checkpoints')\n",
        "if not os.path.exists(ckpt_dir):\n",
        "  os.makedirs(ckpt_dir)\n",
        "  \n",
        "best_acc = 0.\n",
        "ckpt_path = os.path.join(ckpt_dir, 'lastest.pt')\n",
        "if os.path.exists(ckpt_path):\n",
        "  ckpt = torch.load(ckpt_path)\n",
        "  try:\n",
        "    my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer'])\n",
        "    best_acc = ckpt['best_acc']\n",
        "  except RuntimeError as e:\n",
        "      print('wrong checkpoint')\n",
        "  else:    \n",
        "    print('checkpoint is loaded !')\n",
        "    print('current best accuracy : %.2f' % best_acc)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "checkpoint is loaded !\n",
            "current best accuracy : 0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1t7n6yttNEc",
        "colab_type": "text"
      },
      "source": [
        "## Step 8: Train the network\n",
        "\n",
        "Note : It would be better to save checkpoints periodically, otherwise you'll lose everything you've trained if the session is recycled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vczdKbytV38",
        "colab_type": "code",
        "outputId": "82818417-2693-483d-f576-776e0be08131",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        }
      },
      "source": [
        "if training_process:\n",
        "  it = 0\n",
        "  train_losses = []\n",
        "  test_losses = []\n",
        "  for epoch in range(max_epoch):\n",
        "    # train phase\n",
        "    my_classifier.train()\n",
        "    for inputs, labels in train_dataloader:\n",
        "      it += 1\n",
        "\n",
        "      # load data to the GPU.\n",
        "      inputs = inputs.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      # feed data into the network and get outputs.\n",
        "      logits = my_classifier(inputs)\n",
        "\n",
        "      # calculate loss\n",
        "      # Note: `F.cross_entropy` function receives logits, or pre-softmax outputs, rather than final probability scores.\n",
        "      loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "      # Note: You should flush out gradients computed at the previous step before computing gradients at the current step. \n",
        "      #       Otherwise, gradients will accumulate.\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # backprogate loss.\n",
        "      loss.backward()\n",
        "\n",
        "      # update the weights in the network.\n",
        "      optimizer.step()\n",
        "\n",
        "      # calculate accuracy.\n",
        "      acc = (logits.argmax(dim=1) == labels).float().mean()\n",
        "\n",
        "      if it % 2000 == 0:\n",
        "        tbc.save_value('Loss', 'train_loss', it, loss.item())\n",
        "        print('[epoch:{}, iteration:{}] train loss : {:.4f} train accuracy : {:.4f}'.format(epoch, it, loss.item(), acc.item()))\n",
        "\n",
        "    # save losses in a list so that we can visualize them later.\n",
        "    train_losses.append(loss)  \n",
        "\n",
        "    # test phase\n",
        "    n = 0.\n",
        "    test_loss = 0.\n",
        "    test_acc = 0.\n",
        "    my_classifier.eval()\n",
        "    for test_inputs, test_labels in test_dataloader:\n",
        "      test_inputs = test_inputs.to(device)\n",
        "      test_labels = test_labels.to(device)\n",
        "\n",
        "      logits = my_classifier(test_inputs)\n",
        "      test_loss += F.cross_entropy(logits, test_labels, reduction='sum').item()\n",
        "      test_acc += (logits.argmax(dim=1) == test_labels).float().sum().item()\n",
        "      n += inputs.size(0)\n",
        "\n",
        "    test_loss /= n\n",
        "    test_acc /= n\n",
        "    test_losses.append(test_loss)\n",
        "    tbc.save_value('Loss', 'test_loss', it, test_loss)\n",
        "    print('[epoch:{}, iteration:{}] test_loss : {:.4f} test accuracy : {:.4f}'.format(epoch, it, test_loss, test_acc)) \n",
        "\n",
        "    tbc.flush_line('train_loss')\n",
        "    tbc.flush_line('test_loss')\n",
        "\n",
        "    # save checkpoint whenever there is improvement in performance\n",
        "    if test_acc > best_acc:\n",
        "      best_acc = test_acc\n",
        "      # Note: optimizer also has states ! don't forget to save them as well.\n",
        "      ckpt = {'my_classifier':my_classifier.state_dict(),\n",
        "              'optimizer':optimizer.state_dict(),\n",
        "              'best_acc':best_acc}\n",
        "      torch.save(ckpt, ckpt_path)\n",
        "      print('checkpoint is saved !')\n",
        "    \n",
        "tbc.close()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:49: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorboardcolab/core.py:101: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "[epoch:0, iteration:2000] train loss : 1.3674 train accuracy : 0.4444\n",
            "[epoch:0, iteration:2778] test_loss : 1.3600 test accuracy : 0.8165\n",
            "[epoch:1, iteration:4000] train loss : 0.8239 train accuracy : 0.6667\n",
            "[epoch:1, iteration:5556] test_loss : 1.3395 test accuracy : 0.8246\n",
            "[epoch:2, iteration:6000] train loss : 0.8786 train accuracy : 0.6111\n",
            "[epoch:2, iteration:8000] train loss : 0.8532 train accuracy : 0.7778\n",
            "[epoch:2, iteration:8334] test_loss : 1.3374 test accuracy : 0.8234\n",
            "[epoch:3, iteration:10000] train loss : 0.7807 train accuracy : 0.7222\n",
            "[epoch:3, iteration:11112] test_loss : 1.3282 test accuracy : 0.8263\n",
            "[epoch:4, iteration:12000] train loss : 0.3357 train accuracy : 0.8889\n",
            "[epoch:4, iteration:13890] test_loss : 1.3403 test accuracy : 0.8262\n",
            "[epoch:5, iteration:14000] train loss : 1.1688 train accuracy : 0.7222\n",
            "[epoch:5, iteration:16000] train loss : 0.8268 train accuracy : 0.6111\n",
            "[epoch:5, iteration:16668] test_loss : 1.3433 test accuracy : 0.8291\n",
            "[epoch:6, iteration:18000] train loss : 0.7550 train accuracy : 0.7222\n",
            "[epoch:6, iteration:19446] test_loss : 1.3016 test accuracy : 0.8370\n",
            "checkpoint is saved !\n",
            "[epoch:7, iteration:20000] train loss : 0.6013 train accuracy : 0.8333\n",
            "[epoch:7, iteration:22000] train loss : 1.0191 train accuracy : 0.6111\n",
            "[epoch:7, iteration:22224] test_loss : 1.3300 test accuracy : 0.8326\n",
            "[epoch:8, iteration:24000] train loss : 0.5541 train accuracy : 0.7778\n",
            "[epoch:8, iteration:25002] test_loss : 1.3258 test accuracy : 0.8295\n",
            "[epoch:9, iteration:26000] train loss : 0.8931 train accuracy : 0.7222\n",
            "[epoch:9, iteration:27780] test_loss : 1.3150 test accuracy : 0.8289\n",
            "[epoch:10, iteration:28000] train loss : 1.0763 train accuracy : 0.7222\n",
            "[epoch:10, iteration:30000] train loss : 1.0155 train accuracy : 0.6667\n",
            "[epoch:10, iteration:30558] test_loss : 1.3530 test accuracy : 0.8221\n",
            "[epoch:11, iteration:32000] train loss : 1.1474 train accuracy : 0.6667\n",
            "[epoch:11, iteration:33336] test_loss : 1.3648 test accuracy : 0.8231\n",
            "[epoch:12, iteration:34000] train loss : 0.7002 train accuracy : 0.7222\n",
            "[epoch:12, iteration:36000] train loss : 0.5949 train accuracy : 0.8889\n",
            "[epoch:12, iteration:36114] test_loss : 1.3255 test accuracy : 0.8334\n",
            "[epoch:13, iteration:38000] train loss : 0.7354 train accuracy : 0.7222\n",
            "[epoch:13, iteration:38892] test_loss : 1.3626 test accuracy : 0.8240\n",
            "[epoch:14, iteration:40000] train loss : 0.6993 train accuracy : 0.7222\n",
            "[epoch:14, iteration:41670] test_loss : 1.3499 test accuracy : 0.8253\n",
            "[epoch:15, iteration:42000] train loss : 0.7942 train accuracy : 0.6667\n",
            "[epoch:15, iteration:44000] train loss : 0.4864 train accuracy : 0.8333\n",
            "[epoch:15, iteration:44448] test_loss : 1.3345 test accuracy : 0.8295\n",
            "[epoch:16, iteration:46000] train loss : 0.6438 train accuracy : 0.7222\n",
            "[epoch:16, iteration:47226] test_loss : 1.3599 test accuracy : 0.8282\n",
            "[epoch:17, iteration:48000] train loss : 1.4730 train accuracy : 0.4444\n",
            "[epoch:17, iteration:50000] train loss : 0.8576 train accuracy : 0.7778\n",
            "[epoch:17, iteration:50004] test_loss : 1.3419 test accuracy : 0.8261\n",
            "[epoch:18, iteration:52000] train loss : 1.0811 train accuracy : 0.6111\n",
            "[epoch:18, iteration:52782] test_loss : 1.3340 test accuracy : 0.8285\n",
            "[epoch:19, iteration:54000] train loss : 1.0884 train accuracy : 0.5000\n",
            "[epoch:19, iteration:55560] test_loss : 1.3472 test accuracy : 0.8270\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ECu3yS0OvfoR",
        "colab_type": "text"
      },
      "source": [
        "## Step 9: Visualize and analyze the results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G89sqVp-vLRy",
        "colab_type": "code",
        "outputId": "9d3964c5-64dd-4ca0-f11d-9a120f4d3cd7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label='train loss')\n",
        "plt.plot(test_losses, label='test loss')\n",
        "plt.legend()\n",
        "\n",
        "# Re-load trained model\n",
        "my_classifier.load_state_dict(ckpt['my_classifier'])\n",
        "optimizer.load_state_dict(ckpt['optimizer'])\n",
        "\n",
        "# Testing\n",
        "n = 0.\n",
        "test_loss = 0.\n",
        "test_acc = 0.\n",
        "my_classifier.eval()\n",
        "for test_inputs, test_labels in test_dataloader:\n",
        "  test_inputs = test_inputs.to(device)\n",
        "  test_labels = test_labels.to(device)\n",
        "\n",
        "  logits = my_classifier(test_inputs)\n",
        "  test_loss += F.cross_entropy(logits, test_labels, reduction='sum')\n",
        "  test_acc += (logits.argmax(dim=1) == test_labels).float().sum()\n",
        "  n += inputs.size(0)\n",
        "\n",
        "test_loss /= n\n",
        "test_acc /= n\n",
        "print('Test_loss : {:.4f}, Test accuracy : {:.4f}'.format(test_loss.item(), test_acc.item())) "
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test_loss : 1.3472, Test accuracy : 0.8270\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nuCsUCkUSosRdoVAokpD/H3LGUw4C2harAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHjD9SGrvhzc",
        "colab_type": "code",
        "outputId": "67f6d1ee-8d18-4d80-c4fd-43f4cc982805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        }
      },
      "source": [
        "import random\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "my_classifier.eval()\n",
        "\n",
        "num_test_samples = len(test_dataset)\n",
        "random_idx = random.randint(0, num_test_samples)\n",
        "\n",
        "test_input, test_label = test_dataset.__getitem__(random_idx)\n",
        "test_prediction = F.softmax(my_classifier(test_input.unsqueeze(0).to(device)), dim=1).argmax().item()\n",
        "print('label : %s' % classes[test_label])\n",
        "print('prediction : %s' % classes[test_prediction])\n",
        "\n",
        "# functions to show an image\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5     # unnormalize\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "\n",
        "# show images\n",
        "imshow(torchvision.utils.make_grid(test_input))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label : ship\n",
            "prediction : ship\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BwcY8z_C2QLA",
        "colab_type": "text"
      },
      "source": [
        "# <font color=\"blue\"> Discussion and Analysis </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v2W0ZtF82Xyb",
        "colab_type": "text"
      },
      "source": [
        "First, let us briefly summarize/analyze our baseline model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hETE3K0qNn-",
        "colab_type": "text"
      },
      "source": [
        "This part is where the experiment is configured i.e. where the training/optimization/model hyper-parameters are set.\n",
        "\n",
        "Note that our batch_size (to be used in our batch learning later on.) is huge - 20000."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RtZ4k0JqMfg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 20\n",
        "learning_rate = 0.001\n",
        "batch_size = 20000\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7TZ5ZP2qrWYJ",
        "colab_type": "text"
      },
      "source": [
        "This is the given baseline CNN architecture."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTW84JjgrW57",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyClassifier(nn.Module):\n",
        "  def __init__(self):\n",
        "        super(MyClassifier, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5)\n",
        "        self.batchnorm1 = nn.BatchNorm2d(num_features=6)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.conv2 = nn.Conv2d(in_channels=6, out_channels=16, kernel_size=3)\n",
        "        self.conv3 = nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3)\n",
        "        self.batchnorm3 = nn.BatchNorm2d(num_features=32)\n",
        "        self.conv4 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=1)\n",
        "        self.fc1 = nn.Linear(in_features=64 * 1 * 1, out_features=64)\n",
        "        self.fc2 = nn.Linear(in_features=64, out_features=output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = self.pool(self.relu(self.batchnorm1(self.conv1(x))))\n",
        "      x = self.pool(self.relu(self.conv2(x)))\n",
        "      x = self.pool(self.relu(self.batchnorm3(self.conv3(x))))\n",
        "      x = self.pool(self.relu(self.conv4(x)))\n",
        "      x = x.view(-1, 64 * 1 * 1)\n",
        "      x = self.relu(self.fc1(x))\n",
        "      outputs = self.fc2(x)\n",
        "      return outputs"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed08AfwfroAN",
        "colab_type": "text"
      },
      "source": [
        "Lastly, this is the part where the network and optimizer is initialized.\n",
        "\n",
        "Note that we are using Adam as our optimizer. Adam is \"an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments.\" (\"Adam: A Method for Stochastic Optimization\") (*Kingma, Ba*)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4viqpHQOrng4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "my_classifier = MyClassifier()\n",
        "my_classifier = my_classifier.to(device)\n",
        "\n",
        "# Print your neural network structure\n",
        "print(my_classifier)\n",
        "\n",
        "optimizer = optim.Adam(my_classifier.parameters(), lr=learning_rate)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "XToyw2MOw9w9"
      },
      "source": [
        "This baseline model gave a test accuracy of about 0.62 where the accuracies and the graph of training/test loss came out as the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u3aNFfLiBP-",
        "colab_type": "text"
      },
      "source": [
        "[epoch:0, iteration:3] test_loss : 1.0157 test accuracy : 0.6521\n",
        "\n",
        "[epoch:1, iteration:6] test_loss : 1.0145 test accuracy : 0.6521\n",
        "\n",
        "[epoch:2, iteration:9] test_loss : 1.0142 test accuracy : 0.6501\n",
        "\n",
        "[epoch:3, iteration:12] test_loss : 1.0136 test accuracy : 0.6487\n",
        "\n",
        "[epoch:4, iteration:15] test_loss : 1.0120 test accuracy : 0.6485\n",
        "\n",
        "[epoch:5, iteration:18] test_loss : 1.0099 test accuracy : 0.6491\n",
        "\n",
        "[epoch:6, iteration:21] test_loss : 1.0073 test accuracy : 0.6509\n",
        "\n",
        "[epoch:7, iteration:24] test_loss : 1.0049 test accuracy : 0.6522\n",
        "\n",
        "[epoch:8, iteration:27] test_loss : 1.0030 test accuracy : 0.6530\n",
        "\n",
        "[epoch:9, iteration:30] test_loss : 1.0015 test accuracy : 0.6523\n",
        "\n",
        "[epoch:10, iteration:33] test_loss : 1.0005 test accuracy : 0.6530\n",
        "\n",
        "[epoch:11, iteration:36] test_loss : 0.9997 test accuracy : 0.6535\n",
        "\n",
        "[epoch:12, iteration:39] test_loss : 0.9991 test accuracy : 0.6529\n",
        "\n",
        "[epoch:13, iteration:42] test_loss : 0.9987 test accuracy : 0.6534\n",
        "\n",
        "[epoch:14, iteration:45] test_loss : 0.9984 test accuracy : 0.6536\n",
        "\n",
        "[epoch:15, iteration:48] test_loss : 0.9981 test accuracy : 0.6536\n",
        "\n",
        "[epoch:16, iteration:51] test_loss : 0.9979 test accuracy : 0.6542\n",
        "\n",
        "[epoch:17, iteration:54] test_loss : 0.9976 test accuracy : 0.6543\n",
        "\n",
        "[epoch:18, iteration:57] test_loss : 0.9975 test accuracy : 0.6542\n",
        "\n",
        "[epoch:19, iteration:60] test_loss : 0.9976 test accuracy : 0.6549"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIuKnYEVwR2-",
        "colab_type": "text"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://i.gyazo.com/d17c7bb9b1786b41f8d88ae8d5cd785a.png'/>\n",
        "<figcaption>Epoch vs. Accuracy (Batch size: 20000)</figcaption></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbvR7IjIgVU-",
        "colab_type": "text"
      },
      "source": [
        "Now here comes the modification part.\n",
        "\n",
        "Surprisingly, the only modification that I've done to the model was to change the **batch_size** from 20000 (original value) to 18; no change to the basline CNN architecture, no change to learning rate...etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRPEPeBkxG2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# training & optimization hyper-parameters\n",
        "max_epoch = 20\n",
        "learning_rate = 0.001\n",
        "\n",
        "# My modification\n",
        "# batch_size = 20000 <--- Original code\n",
        "batch_size = 18\n",
        "\n",
        "device = 'cuda'\n",
        "\n",
        "# model hyper-parameters\n",
        "output_dim = 10 \n",
        "\n",
        "# Boolean value to select training process\n",
        "training_process = True\n",
        "\n",
        "# initialize tensorboard for visualization\n",
        "# Note : click the Tensorboard link to see the visualization of training/testing results\n",
        "tbc = TensorBoardColab()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cXRWfWJxQH2",
        "colab_type": "text"
      },
      "source": [
        "This gave a test accuracy of about 0.84 where the accuracies and the graph of training/test loss came out as the following:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHMbsak7xV9F",
        "colab_type": "text"
      },
      "source": [
        "[epoch:0, iteration:2000] train loss : 1.3674 train accuracy : 0.4444\n",
        "\n",
        "[epoch:0, iteration:2778] test_loss : 1.3600 test accuracy : 0.8165\n",
        "\n",
        "[epoch:1, iteration:4000] train loss : 0.8239 train accuracy : 0.6667\n",
        "\n",
        "[epoch:1, iteration:5556] test_loss : 1.3395 test accuracy : 0.8246\n",
        "\n",
        "[epoch:2, iteration:6000] train loss : 0.8786 train accuracy : 0.6111\n",
        "\n",
        "[epoch:2, iteration:8000] train loss : 0.8532 train accuracy : 0.7778\n",
        "\n",
        "[epoch:2, iteration:8334] test_loss : 1.3374 test accuracy : 0.8234\n",
        "\n",
        "[epoch:3, iteration:10000] train loss : 0.7807 train accuracy : 0.7222\n",
        "\n",
        "[epoch:3, iteration:11112] test_loss : 1.3282 test accuracy : 0.8263\n",
        "\n",
        "[epoch:4, iteration:12000] train loss : 0.3357 train accuracy : 0.8889\n",
        "\n",
        "[epoch:4, iteration:13890] test_loss : 1.3403 test accuracy : 0.8262\n",
        "\n",
        "[epoch:5, iteration:14000] train loss : 1.1688 train accuracy : 0.7222\n",
        "\n",
        "[epoch:5, iteration:16000] train loss : 0.8268 train accuracy : 0.6111\n",
        "\n",
        "[epoch:5, iteration:16668] test_loss : 1.3433 test accuracy : 0.8291\n",
        "\n",
        "[epoch:6, iteration:18000] train loss : 0.7550 train accuracy : 0.7222\n",
        "\n",
        "[epoch:6, iteration:19446] test_loss : 1.3016 test accuracy : 0.8370\n",
        "\n",
        "checkpoint is saved !\n",
        "\n",
        "[epoch:7, iteration:20000] train loss : 0.6013 train accuracy : 0.8333\n",
        "\n",
        "[epoch:7, iteration:22000] train loss : 1.0191 train accuracy : 0.6111\n",
        "\n",
        "[epoch:7, iteration:22224] test_loss : 1.3300 test accuracy : 0.8326\n",
        "\n",
        "[epoch:8, iteration:24000] train loss : 0.5541 train accuracy : 0.7778\n",
        "\n",
        "[epoch:8, iteration:25002] test_loss : 1.3258 test accuracy : 0.8295\n",
        "\n",
        "[epoch:9, iteration:26000] train loss : 0.8931 train accuracy : 0.7222\n",
        "\n",
        "[epoch:9, iteration:27780] test_loss : 1.3150 test accuracy : 0.8289\n",
        "\n",
        "[epoch:10, iteration:28000] train loss : 1.0763 train accuracy : 0.7222\n",
        "\n",
        "[epoch:10, iteration:30000] train loss : 1.0155 train accuracy : 0.6667\n",
        "\n",
        "[epoch:10, iteration:30558] test_loss : 1.3530 test accuracy : 0.8221\n",
        "\n",
        "[epoch:11, iteration:32000] train loss : 1.1474 train accuracy : 0.6667\n",
        "\n",
        "[epoch:11, iteration:33336] test_loss : 1.3648 test accuracy : 0.8231\n",
        "\n",
        "[epoch:12, iteration:34000] train loss : 0.7002 train accuracy : 0.7222\n",
        "\n",
        "[epoch:12, iteration:36000] train loss : 0.5949 train accuracy : 0.8889\n",
        "\n",
        "[epoch:12, iteration:36114] test_loss : 1.3255 test accuracy : 0.8334\n",
        "\n",
        "[epoch:13, iteration:38000] train loss : 0.7354 train accuracy : 0.7222\n",
        "\n",
        "[epoch:13, iteration:38892] test_loss : 1.3626 test accuracy : 0.8240\n",
        "\n",
        "[epoch:14, iteration:40000] train loss : 0.6993 train accuracy : 0.7222\n",
        "\n",
        "[epoch:14, iteration:41670] test_loss : 1.3499 test accuracy : 0.8253\n",
        "\n",
        "[epoch:15, iteration:42000] train loss : 0.7942 train accuracy : 0.6667\n",
        "\n",
        "[epoch:15, iteration:44000] train loss : 0.4864 train accuracy : 0.8333\n",
        "\n",
        "[epoch:15, iteration:44448] test_loss : 1.3345 test accuracy : 0.8295\n",
        "\n",
        "[epoch:16, iteration:46000] train loss : 0.6438 train accuracy : 0.7222\n",
        "\n",
        "[epoch:16, iteration:47226] test_loss : 1.3599 test accuracy : 0.8282\n",
        "\n",
        "[epoch:17, iteration:48000] train loss : 1.4730 train accuracy : 0.4444\n",
        "\n",
        "[epoch:17, iteration:50000] train loss : 0.8576 train accuracy : 0.7778\n",
        "\n",
        "[epoch:17, iteration:50004] test_loss : 1.3419 test accuracy : 0.8261\n",
        "\n",
        "[epoch:18, iteration:52000] train loss : 1.0811 train accuracy : 0.6111\n",
        "\n",
        "[epoch:18, iteration:52782] test_loss : 1.3340 test accuracy : 0.8285\n",
        "\n",
        "[epoch:19, iteration:54000] train loss : 1.0884 train accuracy : 0.5000\n",
        "\n",
        "[epoch:19, iteration:55560] test_loss : 1.3472 test accuracy : 0.8270\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiR15HMM1l24",
        "colab_type": "text"
      },
      "source": [
        "<figure>\n",
        "<center>\n",
        "<img src='https://i.gyazo.com/c3baa3799370374d1456fc6d896dde0b.png'/>\n",
        "<figcaption>Epoch vs. Accuracy (Batch size: 18)</figcaption></center>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WP9ExKnDxGFn",
        "colab_type": "text"
      },
      "source": [
        "And it turned out that the accuracy in this case exceeded all the other trials that I have done such as trying to change the CNN architecture by adding layers or changing the parameters of each layer.\n",
        "\n",
        "(The order of how I've done this assignment is, admittedly, a bit unorthodox. But I would argue that this is the very result of empirical trials. An analysis and the direction of further studies are outlined below.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5_EE7ylwYnC",
        "colab_type": "text"
      },
      "source": [
        "# Analysis\n",
        "\n",
        "Rather than focusing on the CNN architecture and how to change it (as you might've expected for me to do), I would like to focus on the effect of the size of batch on the accuracy and the model.\n",
        "\n",
        "The metric that I'll use to describe the effect of batch size on training dynamics is the **generalization gap**: the difference between the classfication accuracy at test and training steps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AoQgNti7xxxK",
        "colab_type": "text"
      },
      "source": [
        "Batch learning was introduced to take into account the limited avenue for parallelization for original stochastic gradient methods. As shown in [4], practicing deep-learning people observed that *the generalization gap is rather big when trained with large-batch methods rather than small-batch methods*. But it has not yet been theoretically proven of why this is the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_mhI9NV2Oo4",
        "colab_type": "text"
      },
      "source": [
        "Several researches have been carried out relating to the effect of batch size on the performance of certain model in Deep Learning. One of them, [3], proposed the following conjectures for possible causes of such phenomenon:\n",
        "\n",
        "\n",
        "1.   Large-batch methods overfit the model\n",
        "2.   Large-batch methods are attracted to saddle points\n",
        "3.   Large-batch methods lack the *explorative* properties of small-batch methods and tend to zoom-in on the minimizer closest to the initial point.\n",
        "4.   Small-batch and large-batch methods converge to qualitatively different minimizers with differing generalization properties.\n",
        "\n",
        "And they actually presented some datas that seem to support the last two conjectures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vydFOP7R3rfR",
        "colab_type": "text"
      },
      "source": [
        "The following is the main observation of this paper, and the reason why this little modification made such a big difference in model accuracy:\n",
        "\n",
        "The lack of generalization ability is due to the fact that **large-batch methods tend to convergen to sharp minimizers of the training function** while **small-batch methods tend to convergen to flat minimizers of the training function**\n",
        "\n",
        "*   *Sharp minimizers* are characterized by a significant number of large positive eigenvalues in the Hessian of the training function, and tend to generalize less well as shown in the diagram.\n",
        "*   *Flat minimizers* are characterized by a significant number of small eigenvalues in the Hessian of the training function, and tend to generalize  well, as shown in the diagram.\n",
        "\n",
        "They have observed that the loss function landscape of deep neural networks, such as our CNN, is such that large-batch methods are attracted to regions with sharp minimizers and that, unlike small-batch methods, are unable to escape basins of attraction of these minimizers.\n",
        "\n",
        "<a href=\"https://gyazo.com/a5d8f5f470c196619a7ba6b8e5417dff\"><img src=\"https://i.gyazo.com/a5d8f5f470c196619a7ba6b8e5417dff.png\" alt=\"Image from Gyazo\" width=\"598\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqB3VPBE4CGH",
        "colab_type": "text"
      },
      "source": [
        "Note that flat minimizer can be specified with lower precision than to sharp minimizer. Equivalently, flat minimizers tend to have better generalization performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGYmcWUT4wx_",
        "colab_type": "text"
      },
      "source": [
        "Following is the result of the experiments done in the paper. For all experiments, they've used 10% of the training data as the large-batch size and 256 data points for small-batch size. Also, they've used ADAM optimizer, the same as our model's optimizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRgY4ir75RJc",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://gyazo.com/19441b359b46f7fa990816cab4ad9f97\"><img src=\"https://i.gyazo.com/19441b359b46f7fa990816cab4ad9f97.png\" alt=\"Image from Gyazo\" width=\"586\"/></a>\n",
        "\n",
        "<a href=\"https://gyazo.com/16788fc2c402a6c56aae8aefdaf0356b\"><img src=\"https://i.gyazo.com/16788fc2c402a6c56aae8aefdaf0356b.png\" alt=\"Image from Gyazo\" width=\"593\"/></a>\n",
        "\n",
        "<a href=\"https://gyazo.com/6253804bdbca58040bcc27aed39a8dcd\"><img src=\"https://i.gyazo.com/6253804bdbca58040bcc27aed39a8dcd.png\" alt=\"Image from Gyazo\" width=\"591\"/></a>\n",
        "\n",
        "(Here, SB refers to the small-batch method and LB refers to the large-batch method)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPzMVNh06x2M",
        "colab_type": "text"
      },
      "source": [
        "Above paper([3]), what I think, is the main reason for the drastic improvement of the performance of my model despite having only changed the batch size."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MmrtbjQEnvVw",
        "colab_type": "text"
      },
      "source": [
        "# References / Further Readings\n",
        "[1] https://medium.com/mini-distill/effect-of-batch-size-on-training-dynamics-21c14f7a716e\n",
        "\n",
        "[2] https://stats.stackexchange.com/questions/164876/tradeoff-batch-size-vs-number-of-iterations-to-train-a-neural-network\n",
        "\n",
        "[3] https://arxiv.org/pdf/1609.04836.pdf (\"On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima\") (*Keskar, Mudigere, Nocedal, Smelyanskiy, Tang*)\n",
        "\n",
        "[4] https://cseweb.ucsd.edu/classes/wi08/cse253/Handouts/lecun-98b.pdf (\"Efficient Backdrop\") (*LeCun, Bottou, Orr, Muller*)"
      ]
    }
  ]
}